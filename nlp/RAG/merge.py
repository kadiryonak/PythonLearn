"""
RAG (Retrieval Augmented Generation) MÄ°MARÄ°SÄ° REHBERÄ°
=====================================================

Bu dosya, adÄ±m adÄ±m bir RAG sistemi oluÅŸturmayÄ± Ã¶ÄŸretir.
KodlarÄ± SEN yazacaksÄ±n, ben sadece adÄ±mlarÄ± aÃ§Ä±klÄ±yorum.

RAG NEDÄ°R?
----------
LLM'lere harici bilgi kaynaklarÄ± vererek daha doÄŸru cevaplar Ã¼retmek.
- HalÃ¼sinasyonu azaltÄ±r
- GÃ¼ncel bilgi saÄŸlar
- Ã–zel dokÃ¼manlarÄ± kullanÄ±r

RAG PIPELINE:
    LOAD â†’ SPLIT â†’ EMBED â†’ STORE â†’ RETRIEVE â†’ GENERATE
"""

# ============================================================
# ADIM 0: GEREKLÄ° KÃœTÃœPHANELER
# ============================================================
"""
AÅŸaÄŸÄ±daki kÃ¼tÃ¼phaneleri yÃ¼kle:

pip install langchain langchain-google-genai python-dotenv
pip install langchain-huggingface sentence-transformers
pip install langchain-chroma chromadb
pip install pypdf  # PDF okumak iÃ§in
"""

# TODO: Gerekli import'larÄ± yaz
# import os
# from dotenv import load_dotenv
# load_dotenv()


# ============================================================
# ADIM 1: LOAD (DokÃ¼man YÃ¼kleme)
# ============================================================
"""
FarklÄ± dosya formatlarÄ±ndan veri yÃ¼kle:

1. TXT dosyasÄ± iÃ§in:
   from langchain_community.document_loaders import TextLoader
   loader = TextLoader("dosya.txt", encoding="utf-8")
   documents = loader.load()

2. PDF dosyasÄ± iÃ§in:
   from langchain_community.document_loaders import PyPDFLoader
   loader = PyPDFLoader("dosya.pdf")
   documents = loader.load()

3. Web sayfasÄ± iÃ§in:
   from langchain_community.document_loaders import WebBaseLoader
   loader = WebBaseLoader("https://example.com")
   documents = loader.load()

4. Birden fazla dosya iÃ§in:
   from langchain_community.document_loaders import DirectoryLoader
   loader = DirectoryLoader("./docs", glob="*.txt")
   documents = loader.load()

Her document ÅŸu yapÄ±da:
- page_content: Metnin kendisi
- metadata: {"source": "dosya.txt", "page": 1}
"""

# TODO: Kendi dokÃ¼manÄ±nÄ± yÃ¼kle
# loader = ...
# documents = loader.load()
# print(f"YÃ¼klenen dokÃ¼man sayÄ±sÄ±: {len(documents)}")


# ============================================================
# ADIM 2: SPLIT (Chunking - ParÃ§alama)
# ============================================================
"""
BÃ¼yÃ¼k dokÃ¼manlarÄ± kÃ¼Ã§Ã¼k parÃ§alara bÃ¶l:

from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,      # Her parÃ§a max 500 karakter
    chunk_overlap=100,   # ParÃ§alar arasÄ± 100 karakter tekrar
    separators=["\n\n", "\n", ". ", " ", ""]  # BÃ¶lme Ã¶nceliÄŸi
)

chunks = text_splitter.split_documents(documents)

CHUNK SIZE REHBERÄ°:
- Q&A: 256-512 karakter
- DokÃ¼man arama: 512-1024 karakter
- Ã–zetleme: 1024-2048 karakter

OVERLAP KURALI: chunk_size * 0.1-0.2 (10-20%)
"""

# TODO: DokÃ¼manlarÄ± parÃ§ala
# text_splitter = RecursiveCharacterTextSplitter(...)
# chunks = text_splitter.split_documents(documents)
# print(f"OluÅŸan chunk sayÄ±sÄ±: {len(chunks)}")


# ============================================================
# ADIM 3: EMBED (VektÃ¶re DÃ¶nÃ¼ÅŸtÃ¼rme)
# ============================================================
"""
Metinleri sayÄ±sal vektÃ¶rlere Ã§evir:

SEÃ‡ENEK 1 - HuggingFace (Ãœcretsiz):
from langchain_huggingface import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    model_kwargs={'device': 'cpu'}
)

SEÃ‡ENEK 2 - Google (API Key gerekli):
from langchain_google_genai import GoogleGenerativeAIEmbeddings

embeddings = GoogleGenerativeAIEmbeddings(
    model="models/embedding-001",
    google_api_key=os.getenv("GOOGLE_API_KEY")
)

TEST:
test_vector = embeddings.embed_query("Test metni")
print(f"VektÃ¶r boyutu: {len(test_vector)}")
"""

# TODO: Embedding modelini seÃ§ ve oluÅŸtur
# embeddings = HuggingFaceEmbeddings(...)
# veya
# embeddings = GoogleGenerativeAIEmbeddings(...)


# ============================================================
# ADIM 4: STORE (Vector Database'e Kaydet)
# ============================================================
"""
Chunk'larÄ± ve embedding'lerini veritabanÄ±na kaydet:

from langchain_chroma import Chroma

# Yeni veritabanÄ± oluÅŸtur
vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    collection_name="my_rag_collection",
    persist_directory="./chroma_db"  # Diske kaydet (opsiyonel)
)

# Mevcut veritabanÄ±nÄ± yÃ¼kle
vectorstore = Chroma(
    collection_name="my_rag_collection",
    embedding_function=embeddings,
    persist_directory="./chroma_db"
)

DÄ°ÄER VECTOR DB SEÃ‡ENEKLERÄ°:
- FAISS: HÄ±zlÄ±, yerel
- Pinecone: Bulut, Ã¶lÃ§eklenebilir
- Weaviate: Hibrit arama
- Qdrant: Rust tabanlÄ±, hÄ±zlÄ±
"""

# TODO: Vector store oluÅŸtur
# vectorstore = Chroma.from_documents(...)


# ============================================================
# ADIM 5: RETRIEVE (Benzer DokÃ¼manlarÄ± Getir)
# ============================================================
"""
KullanÄ±cÄ± sorusuna en benzer chunk'larÄ± bul:

YÃ–NTEM 1 - DoÄŸrudan arama:
results = vectorstore.similarity_search(
    query="Sorum nedir?",
    k=3  # En benzer 3 chunk
)

for doc in results:
    print(doc.page_content)

YÃ–NTEM 2 - Skor ile arama:
results = vectorstore.similarity_search_with_score(
    query="Sorum nedir?",
    k=3
)

for doc, score in results:
    print(f"Skor: {score:.4f}")
    print(doc.page_content)

YÃ–NTEM 3 - Retriever olarak kullan:
retriever = vectorstore.as_retriever(
    search_type="similarity",  # veya "mmr"
    search_kwargs={"k": 3}
)

docs = retriever.invoke("Sorum nedir?")
"""

# TODO: Retriever oluÅŸtur ve test et
# retriever = vectorstore.as_retriever(...)
# docs = retriever.invoke("Test sorusu")


# ============================================================
# ADIM 6: GENERATE (LLM ile Cevap Ãœret)
# ============================================================
"""
Bulunan chunk'larÄ± LLM'e vererek cevap Ã¼ret:

YÃ–NTEM 1 - Manuel prompt:
from langchain_google_genai import ChatGoogleGenerativeAI

llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash",
    google_api_key=os.getenv("GOOGLE_API_KEY")
)

# Context oluÅŸtur
context = "\n\n".join([doc.page_content for doc in docs])

# Prompt hazÄ±rla
prompt = f'''AÅŸaÄŸÄ±daki bilgilere dayanarak soruyu cevapla.
Bilgiler dÄ±ÅŸÄ±nda cevap verme.

BÄ°LGÄ°LER:
{context}

SORU: {query}

CEVAP:'''

response = llm.invoke(prompt)
print(response.content)


YÃ–NTEM 2 - RetrievalQA Chain (Otomatik):
from langchain.chains import RetrievalQA

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",  # TÃ¼m chunk'larÄ± birleÅŸtir
    retriever=retriever,
    return_source_documents=True
)

result = qa_chain.invoke({"query": "Sorum nedir?"})
print(result["result"])
print(result["source_documents"])


YÃ–NTEM 3 - Conversational (Sohbet geÃ§miÅŸi ile):
from langchain.chains import ConversationalRetrievalChain

conv_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    return_source_documents=True
)

chat_history = []
result = conv_chain.invoke({
    "question": "Sorum nedir?",
    "chat_history": chat_history
})
"""

# TODO: LLM ve chain oluÅŸtur
# llm = ChatGoogleGenerativeAI(...)
# qa_chain = RetrievalQA.from_chain_type(...)


# ============================================================
# ADIM 7: TAM PIPELINE - HEPSÄ°NÄ° BÄ°RLEÅTÄ°R
# ============================================================
"""
TÃ¼m adÄ±mlarÄ± bir fonksiyonda topla:

def rag_pipeline(query: str) -> str:
    # 1. Benzer dokÃ¼manlarÄ± bul
    docs = retriever.invoke(query)
    
    # 2. Context oluÅŸtur
    context = "\n\n".join([doc.page_content for doc in docs])
    
    # 3. Prompt hazÄ±rla
    prompt = f'''AÅŸaÄŸÄ±daki bilgilere dayanarak cevapla:
    
    {context}
    
    Soru: {query}'''
    
    # 4. LLM'den cevap al
    response = llm.invoke(prompt)
    
    return response.content

# KullanÄ±m
answer = rag_pipeline("Yapay zeka nedir?")
print(answer)
"""

# TODO: Kendi RAG pipeline fonksiyonunu yaz
# def rag_pipeline(query):
#     ...


# ============================================================
# BONUS: STREAMLIT Ä°LE WEB ARAYÃœZÃœ
# ============================================================
"""
pip install streamlit

streamlit_app.py:
-----------------
import streamlit as st

st.title("ğŸ“š RAG Soru-Cevap")

query = st.text_input("Sorunuzu yazÄ±n:")

if query:
    with st.spinner("DÃ¼ÅŸÃ¼nÃ¼yorum..."):
        answer = rag_pipeline(query)
    st.write(answer)

Ã‡alÄ±ÅŸtÄ±r: streamlit run streamlit_app.py
"""


# ============================================================
# Ã–ZET - RAG CHECKLIST
# ============================================================
"""
â–¡ 1. KÃ¼tÃ¼phaneleri yÃ¼kle (pip install ...)
â–¡ 2. .env dosyasÄ±nda API key'i ayarla
â–¡ 3. DokÃ¼manlarÄ± yÃ¼kle (TextLoader, PyPDFLoader)
â–¡ 4. Chunk'lara bÃ¶l (RecursiveCharacterTextSplitter)
â–¡ 5. Embedding modeli seÃ§ (HuggingFace veya Google)
â–¡ 6. Vector DB oluÅŸtur (Chroma)
â–¡ 7. Retriever ayarla (as_retriever)
â–¡ 8. LLM baÄŸla (ChatGoogleGenerativeAI)
â–¡ 9. Pipeline oluÅŸtur (RetrievalQA veya manuel)
â–¡ 10. Test et ve optimize et

Ä°YÄ° Ã‡ALIÅMALAR! ğŸš€
"""
